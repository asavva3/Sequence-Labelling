{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# matplot\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file read\n",
    "\n",
    "def conll_read(path: str):\n",
    "    words = []\n",
    "    bio_tags = []\n",
    "    i = 0\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            i += 1\n",
    "            splitted = line.strip().split('\\t')\n",
    "            if len(splitted) == 1:\n",
    "                if splitted[0] == '':\n",
    "                    words.append('\\n')\n",
    "                    bio_tags.append('\\n')\n",
    "                else:\n",
    "                    # special case for last line of dev set\n",
    "                    # maybe we just skip it altogether\n",
    "                    words.append(splitted[0])\n",
    "                    bio_tags.append('O')\n",
    "            else:\n",
    "                words.append(splitted[0])\n",
    "                bio_tags.append(splitted[1])\n",
    "    return words, bio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls nltk \n",
    "def add_pos_tags(tokens: list):\n",
    "    tagged_train = nltk.pos_tag(tokens)\n",
    "    return zip(*tagged_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the sentences with their features\n",
    "def create_sents(seqs):\n",
    "    seperators = [i for i, item in enumerate(seqs) if item[0] == '\\n']\n",
    "    sents = []\n",
    "    for idx, pos in enumerate(seperators):\n",
    "        start = seperators[idx - 1] + 1\n",
    "        end = seperators[idx]\n",
    "        \n",
    "        if idx == 0:\n",
    "            start = 0\n",
    "            end = pos\n",
    "    \n",
    "        sequence = seqs[start: end]\n",
    "        sents.append(sequence)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "words, bio_tags = conll_read('W-NUT_data/wnut17train.conll')\n",
    "words, pos_tags = add_pos_tags(words)\n",
    "complete = list(zip(words, pos_tags, bio_tags))\n",
    "train_sequences = create_sents(complete)\n",
    "\n",
    "# dev set\n",
    "words, bio_tags = conll_read('W-NUT_data/emerging.dev.conll')\n",
    "words, pos_tags = add_pos_tags(words)\n",
    "complete = list(zip(words, pos_tags, bio_tags))\n",
    "dev_sequences = create_sents(complete)\n",
    "\n",
    "# test set\n",
    "words, bio_tags = conll_read('W-NUT_data/emerging.test.annotated')\n",
    "words, pos_tags = add_pos_tags(words)\n",
    "complete = list(zip(words, pos_tags, bio_tags))\n",
    "test_sequences = create_sents(complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra features\n",
    "is_hashtag_regex = re.compile(r\"#(\\w+)?\")\n",
    "is_mention_regex = re.compile(r\"^@(\\w+)?\")\n",
    "is_money_regex = re.compile(r\"^$(\\w+)?\")\n",
    "is_url_regex = re.compile(r\"(https?:\\/\\/(?:www\\.|(?!www))|www\\.|www\\.)\")\n",
    "is_punct_reg = re.compile(r\"^[\\.\\,!\\?\\\"\\':;_\\-]$\")\n",
    "is_repeated_punct_reg = re.compile(r\"^[\\.\\,!\\?\\\"\\':;_\\-]{2,}$\")\n",
    "is_first_capital_reg = re.compile(r\"^[A-Z][a-z]+\")\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('emoji', first=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def is_hashtag(word: str) -> bool:\n",
    "    return bool(is_hashtag_regex.match(word))\n",
    "\n",
    "def is_mention(word: str) -> bool:\n",
    "    return bool(is_mention_regex.match(word))\n",
    "\n",
    "def is_money(word: str) -> bool:\n",
    "    return bool(is_money_regex.match(word))\n",
    "\n",
    "def is_url(word: str) -> bool:\n",
    "    return bool(is_url_regex.match(word))\n",
    "\n",
    "def is_punct(word: str) -> bool:\n",
    "    return bool(is_punct_reg.match(word))\n",
    "\n",
    "def is_repeated_punct(word: str) -> bool:\n",
    "    return bool(is_repeated_punct_reg.match(word))\n",
    "\n",
    "def is_stopword(word: str) -> bool:\n",
    "    return word.lower() in stop_words_set\n",
    "\n",
    "def is_first_capital(word: str) -> bool:\n",
    "    return bool(is_first_capital_reg.match(word))\n",
    "\n",
    "def has_emoji(word: str) -> bool:\n",
    "    doc = nlp(word)\n",
    "    return doc._.has_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_regex = {\n",
    "                    'is_mention': is_mention_regex,\n",
    "                    'is_money': is_money_regex,\n",
    "                    'is_url': is_url_regex,\n",
    "                    'is_hashtag': is_hashtag_regex,\n",
    "                    'is_punct': is_punct_reg,\n",
    "                    'is_repeated_punct': is_repeated_punct_reg,\n",
    "                    'is_first_capital': is_first_capital_reg,\n",
    "                }\n",
    "\n",
    "def for_features(sent, i, features, features_add, context):\n",
    "    \n",
    "    for feat in features_add:\n",
    "        add_in = ''\n",
    "        word = sent[i][0]\n",
    "        features = add_feature(word, features, feat, add_in)\n",
    "        for c in context:\n",
    "            if c == 0:\n",
    "                continue\n",
    "            if c <= i:\n",
    "                word = sent[i - c][0]\n",
    "                add_in = f'-{c}:'\n",
    "                features = add_feature(word, features, feat, add_in)\n",
    "            if i < (len(sent) - c):\n",
    "                word = sent[i + c][0]\n",
    "                add_in = f'+{c}:'\n",
    "                features = add_feature(word, features, feat, add_in)\n",
    "        \n",
    "    return features\n",
    "\n",
    "def add_feature(word, features, add_feature, context):\n",
    "    if add_feature == 'has_emoji':\n",
    "        feature_value = has_emoji(word)\n",
    "    elif add_feature == 'is_stopword':\n",
    "        feature_value = is_stopword(word)\n",
    "    elif add_feature == 'lemma':\n",
    "        feature_value = lemmatizer.lemmatize(word)\n",
    "    else:\n",
    "        reg = feature_regex[add_feature]\n",
    "        feature_value = bool(reg.match(word))\n",
    "    \n",
    "    features.update({f'{context}word.{add_feature}': feature_value})\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preword2feat(sent, i, extended, extra=None):\n",
    "    features = word2features(sent, i)\n",
    "    if extended:\n",
    "        features = extended_context(features=features, sent=sent, i=i)\n",
    "    if extra:\n",
    "        context = extra[\"context\"]\n",
    "        extra_features = extra[\"features\"]\n",
    "        features = for_features(sent, i, features, extra_features, context)\n",
    "    return features\n",
    "\n",
    "# def add_extra_features(features, sent, i):\n",
    "#     word = sent[i][0]\n",
    "\n",
    "#     features.update({\n",
    "#                         'word.is_hashtag': is_hashtag(word),\n",
    "#                         'word.is_mention': is_mention(word),\n",
    "#                         'word.is_money': is_money(word),\n",
    "#                         'word.is_stopword': is_stopword(word),\n",
    "#                         'word.is_url': is_url(word),\n",
    "#                         'word.is_punct': is_punct(word),\n",
    "#                         'word.is_repeated_punct': is_repeated_punct(word),\n",
    "#                         'word.is_first_capital': is_first_capital(word),\n",
    "#                         'word.has_emoji': has_emoji(word),\n",
    "#                         'word.stem': ps.stem(word),\n",
    "#                         'word.lemma': lemmatizer.lemmatize(word)\n",
    "#                     })\n",
    "#     return features\n",
    "\n",
    "def extended_context(features, sent, i):\n",
    "\n",
    "    if i > 1:\n",
    "        word2 = sent[i-2][0]\n",
    "        postag2 = sent[i-2][1]\n",
    "        features.update({\n",
    "            '-2:word.lower()': word2.lower(),\n",
    "            '-2:word.istitle()': word2.istitle(),\n",
    "            '-2:word.isupper()': word2.isupper(),\n",
    "            '-2:postag': postag2,\n",
    "            '-2:postag[:2]': postag2[:2],\n",
    "        })\n",
    "\n",
    "    if i < len(sent)-2:\n",
    "        word2 = sent[i+2][0]\n",
    "        postag2 = sent[i+2][1]\n",
    "        features.update({\n",
    "            '+2:word.lower()': word2.lower(),\n",
    "            '+2:word.istitle()': word2.istitle(),\n",
    "            '+2:word.isupper()': word2.isupper(),\n",
    "            '+2:postag': postag2,\n",
    "            '+2:postag[:2]': postag2[:2],\n",
    "        })\n",
    "    return features\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent, extended=False, extra=None):\n",
    "    return [preword2feat(sent, i, extended=extended, extra=extra) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y for train, dev, test\n",
    "X_train = [sent2features(s) for s in train_sequences]\n",
    "y_train = [sent2labels(s) for s in train_sequences]\n",
    "\n",
    "X_dev = [sent2features(s) for s in dev_sequences]\n",
    "y_dev = [sent2labels(s) for s in dev_sequences]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sequences]\n",
    "y_test = [sent2labels(s) for s in test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_opt(x: list, y: list, labels: list, params: dict, hyper_params: dict):\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "                                algorithm=params[\"algorithm\"],\n",
    "                                max_iterations=params[\"max_iter\"],\n",
    "                                all_possible_transitions=params[\"poss_trans\"]\n",
    "                              )\n",
    "\n",
    "    # use the same metric for evaluation\n",
    "    f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "    \n",
    "    # search\n",
    "    rs = RandomizedSearchCV(crf, hyper_params,\n",
    "                            cv=3,\n",
    "                            verbose=1,\n",
    "                            n_jobs=-1,\n",
    "                            n_iter=50,\n",
    "                            scoring=f1_scorer,\n",
    "                            random_state=1)\n",
    "\n",
    "    rs.fit(x, y)\n",
    "\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13829694021844366"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreassavva/miniforge3/envs/crf/lib/python3.10/site-packages/sklearn/base.py:193: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   22.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.010529268793042638, 'c2': 0.05466491579572729}\n",
      "best CV score: 0.35136944932248887\n",
      "model size: 0.62M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1921589462256364"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_params = {\n",
    "                \"algorithm\": 'lbfgs',\n",
    "                \"max_iter\": 100,\n",
    "                \"poss_trans\": True\n",
    "            }\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "rs = hyper_param_opt(X_dev, y_dev, labels, rs_params, params_space)\n",
    "\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "\n",
    "best_c1 = rs.best_params_[\"c1\"]\n",
    "best_c2 = rs.best_params_[\"c2\"]\n",
    "\n",
    "y_pred = rs.best_estimator_.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended context\n",
    "X_train_cont = [sent2features(s, True) for s in train_sequences]\n",
    "y_train_cont = [sent2labels(s) for s in train_sequences]\n",
    "\n",
    "X_dev_cont = [sent2features(s, True) for s in dev_sequences]\n",
    "y_dev_cont = [sent2labels(s) for s in dev_sequences]\n",
    "\n",
    "X_test_cont = [sent2features(s, True) for s in test_sequences]\n",
    "y_test_cont = [sent2labels(s) for s in test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13523076229715575"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=best_c1,\n",
    "    c2=best_c2,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train_cont, y_train_cont)\n",
    "\n",
    "y_pred = crf.predict(X_test_cont)\n",
    "metrics.flat_f1_score(y_test_cont, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all extended features\n",
    "extra = {\n",
    "        \"context\": [1],\n",
    "        \"features\": ['is_mention', 'is_money', 'is_url', 'is_hashtag', 'is_punct', 'is_repeated_punct', 'is_first_capital', 'is_stopword', 'has_emoji']\n",
    "        }\n",
    "\n",
    "\n",
    "X_train_ext = [sent2features(s, True, extra) for s in train_sequences]\n",
    "y_train_ext = [sent2labels(s) for s in train_sequences]\n",
    "\n",
    "X_dev_ext = [sent2features(s, True, extra) for s in dev_sequences]\n",
    "y_dev_ext = [sent2labels(s) for s in dev_sequences]\n",
    "\n",
    "X_test_ext = [sent2features(s, True, extra) for s in test_sequences]\n",
    "y_test_ext = [sent2labels(s) for s in test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15784783065542354"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=best_c1,\n",
    "    c2=best_c2,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train_ext, y_train_ext)\n",
    "\n",
    "y_pred = crf.predict(X_test_ext)\n",
    "metrics.flat_f1_score(y_test_ext, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   35.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.022660139525943226, 'c2': 0.02455232191882188}\n",
      "best CV score: 0.38031014853886297\n",
      "model size: 0.79M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21292233712721764"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = hyper_param_opt(X_dev_ext, y_dev_ext, labels, rs_params, params_space)\n",
    "\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "\n",
    "best_c1_ext = rs.best_params_[\"c1\"]\n",
    "best_c2_ext = rs.best_params_[\"c2\"]\n",
    "\n",
    "y_pred = rs.best_estimator_.predict(X_test_ext)\n",
    "metrics.flat_f1_score(y_test_ext, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========---------------------------------------] 22.4% 23.4/104.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 61.4% 64.3/104.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_only = []\n",
    "\n",
    "for sent in train_sequences:\n",
    "    sent_words = []\n",
    "    for w in sent:\n",
    "        sent_words.append(w[0])\n",
    "    sents_only.append(sent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=sents_only, vector_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors.save('models/gensim-twitter-25.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/xwd3ppxs3plg7h09t7vb7cyh0000gn/T/ipykernel_13889/1936859353.py:7: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  _ = glove2word2vec(glove_file, tmp_file)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/andreassavva/miniforge3/envs/crf/lib/python3.10/site-packages/gensim/test/test_data/glove-twitter-25'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [174], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m glove_file \u001b[39m=\u001b[39m datapath(\u001b[39m'\u001b[39m\u001b[39mglove-twitter-25\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m tmp_file \u001b[39m=\u001b[39m get_tmpfile(\u001b[39m\"\u001b[39m\u001b[39mtest_word2vec.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m _ \u001b[39m=\u001b[39m glove2word2vec(glove_file, tmp_file)\n",
      "File \u001b[0;32m~/miniforge3/envs/crf/lib/python3.10/site-packages/gensim/utils.py:1522\u001b[0m, in \u001b[0;36mdeprecated.<locals>.decorator.<locals>.new_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m   1516\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_func1\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1517\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1518\u001b[0m         fmt\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, reason\u001b[39m=\u001b[39mreason),\n\u001b[1;32m   1519\u001b[0m         category\u001b[39m=\u001b[39m\u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1520\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m   1521\u001b[0m     )\n\u001b[0;32m-> 1522\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/crf/lib/python3.10/site-packages/gensim/scripts/glove2word2vec.py:109\u001b[0m, in \u001b[0;36mglove2word2vec\u001b[0;34m(glove_input_file, word2vec_output_file)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m@deprecated\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mKeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mglove2word2vec\u001b[39m(glove_input_file, word2vec_output_file):\n\u001b[1;32m     94\u001b[0m     \u001b[39m\"\"\"Convert `glove_input_file` in GloVe format to word2vec format and write it to `word2vec_output_file`.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     glovekv \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(glove_input_file, binary\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, no_header\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    111\u001b[0m     num_lines, num_dims \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(glovekv), glovekv\u001b[39m.\u001b[39mvector_size\n\u001b[1;32m    112\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mconverting \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m vectors from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, num_lines, glove_input_file, word2vec_output_file)\n",
      "File \u001b[0;32m~/miniforge3/envs/crf/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1723\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1678\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1679\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1680\u001b[0m     ):\n\u001b[1;32m   1681\u001b[0m     \u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \n\u001b[1;32m   1683\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \n\u001b[1;32m   1722\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1724\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1725\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1726\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/crf/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2052\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2049\u001b[0m             counts[word] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(count)\n\u001b[1;32m   2051\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n\u001b[0;32m-> 2052\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m   2053\u001b[0m     \u001b[39mif\u001b[39;00m no_header:\n\u001b[1;32m   2054\u001b[0m         \u001b[39m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m         \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/miniforge3/envs/crf/lib/python3.10/site-packages/smart_open/smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 188\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[1;32m    189\u001b[0m     uri,\n\u001b[1;32m    190\u001b[0m     mode,\n\u001b[1;32m    191\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    192\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[1;32m    193\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    194\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    195\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    196\u001b[0m )\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/miniforge3/envs/crf/lib/python3.10/site-packages/smart_open/smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    359\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[0;32m--> 361\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39;49mbuffering, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/andreassavva/miniforge3/envs/crf/lib/python3.10/site-packages/gensim/test/test_data/glove-twitter-25'"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = datapath('glove-twitter-25')\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_word2vec.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [170], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_2 \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mKeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(\u001b[39m'\u001b[39;49m\u001b[39mtest_word2vec.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, binary\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, unicode_errors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mignore\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/crf/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1723\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1678\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1679\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1680\u001b[0m     ):\n\u001b[1;32m   1681\u001b[0m     \u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \n\u001b[1;32m   1683\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \n\u001b[1;32m   1722\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1724\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1725\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1726\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/crf/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2052\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2049\u001b[0m             counts[word] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(count)\n\u001b[1;32m   2051\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n\u001b[0;32m-> 2052\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m   2053\u001b[0m     \u001b[39mif\u001b[39;00m no_header:\n\u001b[1;32m   2054\u001b[0m         \u001b[39m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m         \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/miniforge3/envs/crf/lib/python3.10/site-packages/smart_open/smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 188\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[1;32m    189\u001b[0m     uri,\n\u001b[1;32m    190\u001b[0m     mode,\n\u001b[1;32m    191\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    192\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[1;32m    193\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    194\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    195\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    196\u001b[0m )\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/miniforge3/envs/crf/lib/python3.10/site-packages/smart_open/smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    359\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[0;32m--> 361\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39;49mbuffering, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_word2vec.txt'"
     ]
    }
   ],
   "source": [
    "model_2 = gensim.models.KeyedVectors.load_word2vec_format('test_word2vec.txt', binary=False, unicode_errors='ignore')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('crf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50f95997f0c4282c02b70176f376c0566cd16eb2ce4c64de3c8feb9eb1d0f610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
