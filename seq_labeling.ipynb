{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# matplot\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file read\n",
    "\n",
    "def conll_read(path: str):\n",
    "    words = []\n",
    "    bio_tags = []\n",
    "    i = 0\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            i += 1\n",
    "            splitted = line.strip().split('\\t')\n",
    "            if len(splitted) == 1:\n",
    "                if splitted[0] == '':\n",
    "                    words.append('\\n')\n",
    "                    bio_tags.append('\\n')\n",
    "                else:\n",
    "                    # special case for last line of dev set\n",
    "                    # maybe we just skip it altogether\n",
    "                    words.append(splitted[0])\n",
    "                    bio_tags.append('O')\n",
    "            else:\n",
    "                words.append(splitted[0])\n",
    "                bio_tags.append(splitted[1])\n",
    "    return words, bio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls nltk \n",
    "def add_pos_tags(tokens: list):\n",
    "    tagged_train = nltk.pos_tag(tokens)\n",
    "    return zip(*tagged_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the sentences with their features\n",
    "def create_sents(seqs):\n",
    "    seperators = [i for i, item in enumerate(seqs) if item[0] == '\\n']\n",
    "    sents = []\n",
    "    for idx, pos in enumerate(seperators):\n",
    "        start = seperators[idx - 1] + 1\n",
    "        end = seperators[idx]\n",
    "        \n",
    "        if idx == 0:\n",
    "            start = 0\n",
    "            end = pos\n",
    "    \n",
    "        sequence = seqs[start: end]\n",
    "        sents.append(sequence)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "words, bio_tags = conll_read('W-NUT_data/wnut17train.conll')\n",
    "words, pos_tags = add_pos_tags(words)\n",
    "complete = list(zip(words, pos_tags, bio_tags))\n",
    "train_sequences = create_sents(complete)\n",
    "\n",
    "# dev set\n",
    "words, bio_tags = conll_read('W-NUT_data/emerging.dev.conll')\n",
    "words, pos_tags = add_pos_tags(words)\n",
    "complete = list(zip(words, pos_tags, bio_tags))\n",
    "dev_sequences = create_sents(complete)\n",
    "\n",
    "# test set\n",
    "words, bio_tags = conll_read('W-NUT_data/emerging.test.annotated')\n",
    "words, pos_tags = add_pos_tags(words)\n",
    "complete = list(zip(words, pos_tags, bio_tags))\n",
    "test_sequences = create_sents(complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra features\n",
    "is_hashtag_regex = re.compile(r\"#(\\w+)?\")\n",
    "is_mention_regex = re.compile(r\"^@(\\w+)?\")\n",
    "is_money_regex = re.compile(r\"^$(\\w+)?\")\n",
    "is_url_regex = re.compile(r\"(https?:\\/\\/(?:www\\.|(?!www))|www\\.|www\\.)\")\n",
    "is_punct_reg = re.compile(r\"^[\\.\\,!\\?\\\"\\':;_\\-]$\")\n",
    "is_repeated_punct_reg = re.compile(r\"^[\\.\\,!\\?\\\"\\':;_\\-]{2,}$\")\n",
    "is_first_capital_reg = re.compile(r\"^[A-Z][a-z]+\")\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('emoji', first=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def is_hashtag(word: str) -> bool:\n",
    "    return bool(is_hashtag_regex.match(word))\n",
    "\n",
    "def is_mention(word: str) -> bool:\n",
    "    return bool(is_mention_regex.match(word))\n",
    "\n",
    "def is_money(word: str) -> bool:\n",
    "    return bool(is_money_regex.match(word))\n",
    "\n",
    "def is_url(word: str) -> bool:\n",
    "    return bool(is_url_regex.match(word))\n",
    "\n",
    "def is_punct(word: str) -> bool:\n",
    "    return bool(is_punct_reg.match(word))\n",
    "\n",
    "def is_repeated_punct(word: str) -> bool:\n",
    "    return bool(is_repeated_punct_reg.match(word))\n",
    "\n",
    "def is_stopword(word: str) -> bool:\n",
    "    return word.lower() in stop_words_set\n",
    "\n",
    "def is_first_capital(word: str) -> bool:\n",
    "    return bool(is_first_capital_reg.match(word))\n",
    "\n",
    "def has_emoji(word: str) -> bool:\n",
    "    doc = nlp(word)\n",
    "    return doc._.has_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_regex = {\n",
    "                    'is_mention': is_mention_regex,\n",
    "                    'is_money': is_money_regex,\n",
    "                    'is_url': is_url_regex,\n",
    "                    'is_hashtag': is_hashtag_regex,\n",
    "                    'is_punct': is_punct_reg,\n",
    "                    'is_repeated_punct': is_repeated_punct_reg,\n",
    "                    'is_first_capital': is_first_capital_reg,\n",
    "                }\n",
    "\n",
    "def for_features(sent, i, features, features_add, context):\n",
    "    \n",
    "    for feat in features_add:\n",
    "        add_in = ''\n",
    "        word = sent[i][0]\n",
    "        features = add_feature(word, features, feat, add_in)\n",
    "        for c in context:\n",
    "            if c == 0:\n",
    "                continue\n",
    "            if c <= i:\n",
    "                word = sent[i - c][0]\n",
    "                add_in = f'-{c}:'\n",
    "                features = add_feature(word, features, feat, add_in)\n",
    "            if i < (len(sent) - c):\n",
    "                word = sent[i + c][0]\n",
    "                add_in = f'+{c}:'\n",
    "                features = add_feature(word, features, feat, add_in)\n",
    "        \n",
    "    return features\n",
    "\n",
    "def add_feature(word, features, add_feature, context):\n",
    "    if add_feature == 'has_emoji':\n",
    "        feature_value = has_emoji(word)\n",
    "    elif add_feature == 'is_stopword':\n",
    "        feature_value = is_stopword(word)\n",
    "    elif add_feature == 'lemma':\n",
    "        feature_value = lemmatizer.lemmatize(word)\n",
    "    else:\n",
    "        reg = feature_regex[add_feature]\n",
    "        feature_value = bool(reg.match(word))\n",
    "    \n",
    "    features.update({f'{context}word.{add_feature}': feature_value})\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preword2feat(sent, i, extended, extra=None):\n",
    "    features = word2features(sent, i)\n",
    "    if extended:\n",
    "        features = extended_context(features=features, sent=sent, i=i)\n",
    "    if extra:\n",
    "        context = extra[\"context\"]\n",
    "        extra_features = extra[\"features\"]\n",
    "        features = for_features(sent, i, features, extra_features, context)\n",
    "    return features\n",
    "\n",
    "# def add_extra_features(features, sent, i):\n",
    "#     word = sent[i][0]\n",
    "\n",
    "#     features.update({\n",
    "#                         'word.is_hashtag': is_hashtag(word),\n",
    "#                         'word.is_mention': is_mention(word),\n",
    "#                         'word.is_money': is_money(word),\n",
    "#                         'word.is_stopword': is_stopword(word),\n",
    "#                         'word.is_url': is_url(word),\n",
    "#                         'word.is_punct': is_punct(word),\n",
    "#                         'word.is_repeated_punct': is_repeated_punct(word),\n",
    "#                         'word.is_first_capital': is_first_capital(word),\n",
    "#                         'word.has_emoji': has_emoji(word),\n",
    "#                         'word.stem': ps.stem(word),\n",
    "#                         'word.lemma': lemmatizer.lemmatize(word)\n",
    "#                     })\n",
    "#     return features\n",
    "\n",
    "def extended_context(features, sent, i):\n",
    "\n",
    "    if i > 1:\n",
    "        word2 = sent[i-2][0]\n",
    "        postag2 = sent[i-2][1]\n",
    "        features.update({\n",
    "            '-2:word.lower()': word2.lower(),\n",
    "            '-2:word.istitle()': word2.istitle(),\n",
    "            '-2:word.isupper()': word2.isupper(),\n",
    "            '-2:postag': postag2,\n",
    "            '-2:postag[:2]': postag2[:2],\n",
    "        })\n",
    "\n",
    "    if i < len(sent)-2:\n",
    "        word2 = sent[i+2][0]\n",
    "        postag2 = sent[i+2][1]\n",
    "        features.update({\n",
    "            '+2:word.lower()': word2.lower(),\n",
    "            '+2:word.istitle()': word2.istitle(),\n",
    "            '+2:word.isupper()': word2.isupper(),\n",
    "            '+2:postag': postag2,\n",
    "            '+2:postag[:2]': postag2[:2],\n",
    "        })\n",
    "    return features\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent, extended=False, extra=None):\n",
    "    return [preword2feat(sent, i, extended=extended, extra=extra) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y for train, dev, test\n",
    "X_train = [sent2features(s) for s in train_sequences]\n",
    "y_train = [sent2labels(s) for s in train_sequences]\n",
    "\n",
    "X_dev = [sent2features(s) for s in dev_sequences]\n",
    "y_dev = [sent2labels(s) for s in dev_sequences]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sequences]\n",
    "y_test = [sent2labels(s) for s in test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_opt(x: list, y: list, labels: list, params: dict, hyper_params: dict):\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "                                algorithm=params[\"algorithm\"],\n",
    "                                max_iterations=params[\"max_iter\"],\n",
    "                                all_possible_transitions=params[\"poss_trans\"]\n",
    "                              )\n",
    "\n",
    "    # use the same metric for evaluation\n",
    "    f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "    \n",
    "    # search\n",
    "    rs = RandomizedSearchCV(crf, hyper_params,\n",
    "                            cv=3,\n",
    "                            verbose=1,\n",
    "                            n_jobs=-1,\n",
    "                            n_iter=50,\n",
    "                            scoring=f1_scorer,\n",
    "                            random_state=1)\n",
    "\n",
    "    rs.fit(x, y)\n",
    "\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13829694021844366"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreassavva/miniforge3/envs/crf/lib/python3.10/site-packages/sklearn/base.py:193: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   22.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.010529268793042638, 'c2': 0.05466491579572729}\n",
      "best CV score: 0.35136944932248887\n",
      "model size: 0.62M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1921589462256364"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_params = {\n",
    "                \"algorithm\": 'lbfgs',\n",
    "                \"max_iter\": 100,\n",
    "                \"poss_trans\": True\n",
    "            }\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "rs = hyper_param_opt(X_dev, y_dev, labels, rs_params, params_space)\n",
    "\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "\n",
    "best_c1 = rs.best_params_[\"c1\"]\n",
    "best_c2 = rs.best_params_[\"c2\"]\n",
    "\n",
    "y_pred = rs.best_estimator_.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended context\n",
    "X_train_cont = [sent2features(s, True) for s in train_sequences]\n",
    "y_train_cont = [sent2labels(s) for s in train_sequences]\n",
    "\n",
    "X_dev_cont = [sent2features(s, True) for s in dev_sequences]\n",
    "y_dev_cont = [sent2labels(s) for s in dev_sequences]\n",
    "\n",
    "X_test_cont = [sent2features(s, True) for s in test_sequences]\n",
    "y_test_cont = [sent2labels(s) for s in test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13523076229715575"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=best_c1,\n",
    "    c2=best_c2,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train_cont, y_train_cont)\n",
    "\n",
    "y_pred = crf.predict(X_test_cont)\n",
    "metrics.flat_f1_score(y_test_cont, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all extended features\n",
    "extra = {\n",
    "        \"context\": [1],\n",
    "        \"features\": ['is_mention', 'is_money', 'is_url', 'is_hashtag', 'is_punct', 'is_repeated_punct', 'is_first_capital', 'is_stopword', 'has_emoji']\n",
    "        }\n",
    "\n",
    "\n",
    "X_train_ext = [sent2features(s, True, extra) for s in train_sequences]\n",
    "y_train_ext = [sent2labels(s) for s in train_sequences]\n",
    "\n",
    "X_dev_ext = [sent2features(s, True, extra) for s in dev_sequences]\n",
    "y_dev_ext = [sent2labels(s) for s in dev_sequences]\n",
    "\n",
    "X_test_ext = [sent2features(s, True, extra) for s in test_sequences]\n",
    "y_test_ext = [sent2labels(s) for s in test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15784783065542354"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=best_c1,\n",
    "    c2=best_c2,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train_ext, y_train_ext)\n",
    "\n",
    "y_pred = crf.predict(X_test_ext)\n",
    "metrics.flat_f1_score(y_test_ext, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   35.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.022660139525943226, 'c2': 0.02455232191882188}\n",
      "best CV score: 0.38031014853886297\n",
      "model size: 0.79M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21292233712721764"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = hyper_param_opt(X_dev_ext, y_dev_ext, labels, rs_params, params_space)\n",
    "\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "\n",
    "best_c1_ext = rs.best_params_[\"c1\"]\n",
    "best_c2_ext = rs.best_params_[\"c2\"]\n",
    "\n",
    "y_pred = rs.best_estimator_.predict(X_test_ext)\n",
    "metrics.flat_f1_score(y_test_ext, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========---------------------------------------] 22.4% 23.4/104.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 61.4% 64.3/104.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# import gensim.downloader\n",
    "# glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_only = []\n",
    "\n",
    "for sent in train_sequences:\n",
    "    sent_words = []\n",
    "    for w in sent:\n",
    "        sent_words.append(w[0])\n",
    "    sents_only.append(sent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec \n",
    "\n",
    "model = Word2Vec(sents_only, vector_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_examples = model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/xwd3ppxs3plg7h09t7vb7cyh0000gn/T/ipykernel_2757/2922512917.py:7: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  _ = glove2word2vec(glove_file, tmp_file)\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = 'models/glove.twitter.27B.25d.txt'\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "pre_trained = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab([list(pre_trained.index_to_key)], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model.wv.vectors_lockf = np.ones(len(model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.intersect_word2vec_format(tmp_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635558, 1254600)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sents_only, total_examples=total_examples, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " ',',\n",
       " ':',\n",
       " 'the',\n",
       " 'to',\n",
       " 'I',\n",
       " 'a',\n",
       " '!',\n",
       " 'RT',\n",
       " 'and',\n",
       " 'you',\n",
       " 'in',\n",
       " '...',\n",
       " 'for',\n",
       " 'on',\n",
       " 'it',\n",
       " 'of',\n",
       " \"'s\",\n",
       " 'is',\n",
       " 'at',\n",
       " 'my',\n",
       " '?',\n",
       " 'be',\n",
       " 'when',\n",
       " 'day',\n",
       " 'i',\n",
       " '\"',\n",
       " 'time',\n",
       " 'that',\n",
       " 'me',\n",
       " 'today',\n",
       " 'tomorrow',\n",
       " 'have',\n",
       " 'with',\n",
       " '-',\n",
       " 'this',\n",
       " 'tonight',\n",
       " 'up',\n",
       " 'out',\n",
       " 'will',\n",
       " 'The',\n",
       " 'get',\n",
       " 'but',\n",
       " 'was',\n",
       " 'your',\n",
       " '(',\n",
       " 'all',\n",
       " 'just',\n",
       " ':)',\n",
       " 'so',\n",
       " 'may',\n",
       " 'week',\n",
       " 'not',\n",
       " '..',\n",
       " \"I'm\",\n",
       " ')',\n",
       " '&amp;',\n",
       " 'like',\n",
       " '!!',\n",
       " 'from',\n",
       " 'u',\n",
       " 'do',\n",
       " 'are',\n",
       " 'go',\n",
       " 'we',\n",
       " 'can',\n",
       " '2',\n",
       " 'one',\n",
       " 'know',\n",
       " 'Friday',\n",
       " '3',\n",
       " 'about',\n",
       " '&lt;',\n",
       " 'now',\n",
       " 'It',\n",
       " \"'\",\n",
       " 'night',\n",
       " 'after',\n",
       " 'A',\n",
       " 'by',\n",
       " 'he',\n",
       " 'Sunday',\n",
       " 'going',\n",
       " 'before',\n",
       " '2015',\n",
       " 'good',\n",
       " 'some',\n",
       " 'got',\n",
       " 'there',\n",
       " 'as',\n",
       " 'been',\n",
       " 'if',\n",
       " 'they',\n",
       " 'see',\n",
       " 'lol',\n",
       " 'January',\n",
       " 'next',\n",
       " 'or',\n",
       " 'what',\n",
       " 'an',\n",
       " 'last',\n",
       " 'its',\n",
       " 'You',\n",
       " 'off',\n",
       " 'days',\n",
       " 'our',\n",
       " 'love',\n",
       " '!!!',\n",
       " 'back',\n",
       " 'no',\n",
       " 'When',\n",
       " 'need',\n",
       " 'years',\n",
       " 'year',\n",
       " 'had',\n",
       " 'has',\n",
       " 'want',\n",
       " 'school',\n",
       " 'Day',\n",
       " \"don't\",\n",
       " 'her',\n",
       " 'more',\n",
       " 'his',\n",
       " 'think',\n",
       " 'work',\n",
       " 'December',\n",
       " 'come',\n",
       " 'new',\n",
       " 'who',\n",
       " 'would',\n",
       " ':D',\n",
       " 'This',\n",
       " 'show',\n",
       " 'people',\n",
       " '....',\n",
       " 'too',\n",
       " 'Saturday',\n",
       " '1',\n",
       " 'im',\n",
       " 'well',\n",
       " 'again',\n",
       " '2014',\n",
       " 'We',\n",
       " 'make',\n",
       " 'game',\n",
       " 'life',\n",
       " 'AM',\n",
       " 'them',\n",
       " 'long',\n",
       " '|',\n",
       " 'then',\n",
       " 'home',\n",
       " '1st',\n",
       " 'she',\n",
       " 'PM',\n",
       " '4',\n",
       " 'him',\n",
       " 'really',\n",
       " 'What',\n",
       " 'gonna',\n",
       " 'say',\n",
       " 'May',\n",
       " 'even',\n",
       " 'how',\n",
       " 'If',\n",
       " 'over',\n",
       " 'their',\n",
       " 'Today',\n",
       " 'here',\n",
       " 'still',\n",
       " 'And',\n",
       " 'My',\n",
       " 'Jan',\n",
       " 'first',\n",
       " '&quot;',\n",
       " 'So',\n",
       " 'Just',\n",
       " 'every',\n",
       " 'New',\n",
       " 'were',\n",
       " 'Happy',\n",
       " 'Monday',\n",
       " 'much',\n",
       " 'us',\n",
       " 'right',\n",
       " 'weekend',\n",
       " 'great',\n",
       " '[',\n",
       " '--',\n",
       " 'than',\n",
       " 'Time',\n",
       " 'wait',\n",
       " ']',\n",
       " 'only',\n",
       " 'should',\n",
       " 'start',\n",
       " 'hope',\n",
       " 'did',\n",
       " \"I'll\",\n",
       " 'always',\n",
       " 'No',\n",
       " 'friday',\n",
       " 'better',\n",
       " 'something',\n",
       " 'via',\n",
       " 'ago',\n",
       " 'am',\n",
       " 'Tuesday',\n",
       " \"I've\",\n",
       " 'Good',\n",
       " 'feel',\n",
       " '2010',\n",
       " 'More',\n",
       " 'made',\n",
       " ';)',\n",
       " '@',\n",
       " '10',\n",
       " 'YOU',\n",
       " '2nd',\n",
       " 'into',\n",
       " 'Is',\n",
       " 'party',\n",
       " 'said',\n",
       " ':(',\n",
       " '&gt;',\n",
       " 'ur',\n",
       " 'ready',\n",
       " 'x',\n",
       " 'could',\n",
       " \"can't\",\n",
       " 'few',\n",
       " 'hate',\n",
       " 'look',\n",
       " 'someone',\n",
       " 'weeks',\n",
       " 'excited',\n",
       " 'take',\n",
       " 'way',\n",
       " 'birthday',\n",
       " 'After',\n",
       " '6',\n",
       " 'sleep',\n",
       " \"you're\",\n",
       " 'happy',\n",
       " 'live',\n",
       " '??',\n",
       " 'any',\n",
       " '17',\n",
       " '!!!!',\n",
       " 'done',\n",
       " 'sure',\n",
       " 'already',\n",
       " 'let',\n",
       " 'coming',\n",
       " 'down',\n",
       " 'bad',\n",
       " 'because',\n",
       " ';',\n",
       " 'oh',\n",
       " '5',\n",
       " 'Have',\n",
       " 'never',\n",
       " 'two',\n",
       " 'Tomorrow',\n",
       " 'On',\n",
       " 'How',\n",
       " 'morning',\n",
       " 'until',\n",
       " 'around',\n",
       " \"n't\",\n",
       " 'thing',\n",
       " 'doing',\n",
       " 'He',\n",
       " 'Wednesday',\n",
       " 'why',\n",
       " 'call',\n",
       " 'took',\n",
       " 'getting',\n",
       " 'February',\n",
       " \"'m\",\n",
       " 'same',\n",
       " 'other',\n",
       " 'best',\n",
       " \"i'm\",\n",
       " 'September',\n",
       " 'tired',\n",
       " 'Week',\n",
       " 'free',\n",
       " 'miss',\n",
       " '$',\n",
       " 'where',\n",
       " 'check',\n",
       " 'For',\n",
       " 'friends',\n",
       " 'tweet',\n",
       " 'very',\n",
       " 'THE',\n",
       " '20',\n",
       " '7',\n",
       " 'fun',\n",
       " 'remember',\n",
       " 'U',\n",
       " 'being',\n",
       " 'ME',\n",
       " 'though',\n",
       " 'find',\n",
       " 'please',\n",
       " 'But',\n",
       " '8',\n",
       " 'Love',\n",
       " 'play',\n",
       " 'Justin',\n",
       " 'away',\n",
       " 'video',\n",
       " 'ever',\n",
       " 'later',\n",
       " 'Thanks',\n",
       " '15',\n",
       " 'old',\n",
       " 'world',\n",
       " 'everyone',\n",
       " 'N',\n",
       " 'In',\n",
       " 'through',\n",
       " 'haha',\n",
       " 'tell',\n",
       " 'Twitter',\n",
       " 'AND',\n",
       " 'ON',\n",
       " 'yesterday',\n",
       " 'TO',\n",
       " 'class',\n",
       " 'mon',\n",
       " 'All',\n",
       " 'season',\n",
       " 'since',\n",
       " 'That',\n",
       " 'end',\n",
       " '25',\n",
       " 'Come',\n",
       " 'amazing',\n",
       " 'follow',\n",
       " 'Try',\n",
       " 'Well',\n",
       " 'give',\n",
       " 'wanna',\n",
       " 'October',\n",
       " 'month',\n",
       " 'Not',\n",
       " 'says',\n",
       " 'big',\n",
       " 'tonite',\n",
       " 'shit',\n",
       " 'n',\n",
       " '.\"',\n",
       " 'most',\n",
       " '3rd',\n",
       " 'Sept',\n",
       " 'job',\n",
       " 'try',\n",
       " 'found',\n",
       " '...\"',\n",
       " 'na',\n",
       " 'went',\n",
       " 'yet',\n",
       " 'sun',\n",
       " 'help',\n",
       " 'o',\n",
       " 'WHEN',\n",
       " '18',\n",
       " 'nice',\n",
       " 'God',\n",
       " 'saw',\n",
       " 'Hope',\n",
       " 'Thursday',\n",
       " 'feeling',\n",
       " 'little',\n",
       " 'having',\n",
       " 'Tonight',\n",
       " 'concert',\n",
       " 'place',\n",
       " 'hair',\n",
       " 'Part',\n",
       " 'might',\n",
       " 'Date',\n",
       " 'win',\n",
       " 'ask',\n",
       " 'Oh',\n",
       " 'everything',\n",
       " 'things',\n",
       " 'during',\n",
       " 'early',\n",
       " 'own',\n",
       " 'IT',\n",
       " 'face',\n",
       " 'leave',\n",
       " 'gets',\n",
       " 'LOL',\n",
       " '@null',\n",
       " 'story',\n",
       " 'sunday',\n",
       " 'another',\n",
       " 'stop',\n",
       " 'man',\n",
       " 'pm',\n",
       " 'yeah',\n",
       " 'set',\n",
       " 'head',\n",
       " 'Let',\n",
       " 'hard',\n",
       " 'looking',\n",
       " 'Our',\n",
       " 'Sun',\n",
       " 'half',\n",
       " 'against',\n",
       " 'crazy',\n",
       " 'believe',\n",
       " 'watch',\n",
       " '30',\n",
       " '11',\n",
       " 'baby',\n",
       " 'Get',\n",
       " 'chance',\n",
       " 'com',\n",
       " 'house',\n",
       " 'kids',\n",
       " 'late',\n",
       " 'Do',\n",
       " '.....',\n",
       " 'must',\n",
       " 'Why',\n",
       " 'There',\n",
       " 'pretty',\n",
       " 'awesome',\n",
       " 'months',\n",
       " 'stay',\n",
       " 'Check',\n",
       " 'bed',\n",
       " 'forget',\n",
       " 'high',\n",
       " 'Feb',\n",
       " 'IS',\n",
       " 'followers',\n",
       " 'Lol',\n",
       " 'full',\n",
       " 'these',\n",
       " 'Now',\n",
       " 'Of',\n",
       " 'twitter',\n",
       " 'goes',\n",
       " 'maybe',\n",
       " '+',\n",
       " 'guys',\n",
       " 'FRIDAY',\n",
       " 'Your',\n",
       " '12',\n",
       " 'seen',\n",
       " 'playing',\n",
       " 'With',\n",
       " 'heart',\n",
       " 'Who',\n",
       " 'ya',\n",
       " 'cant',\n",
       " 'actually',\n",
       " 'Dec',\n",
       " '@justinbieber',\n",
       " 'de',\n",
       " 'put',\n",
       " 'dont',\n",
       " 'dad',\n",
       " '0',\n",
       " 'mom',\n",
       " 'Here',\n",
       " 'smile',\n",
       " 'da',\n",
       " \"Can't\",\n",
       " 'till',\n",
       " 'starts',\n",
       " 'music',\n",
       " 'March',\n",
       " 'Bieber',\n",
       " '#FF',\n",
       " 'catch',\n",
       " 'cool',\n",
       " 'They',\n",
       " 'One',\n",
       " 'Haha',\n",
       " 'word',\n",
       " 'read',\n",
       " '?!',\n",
       " 'list',\n",
       " 'run',\n",
       " 'meet',\n",
       " 'To',\n",
       " 'comes',\n",
       " 'thought',\n",
       " 'left',\n",
       " 'watching',\n",
       " 'soon',\n",
       " 'L',\n",
       " 'yes',\n",
       " 'minutes',\n",
       " 'lot',\n",
       " 'c',\n",
       " 'text',\n",
       " 'She',\n",
       " '=',\n",
       " 'vs',\n",
       " 'dinner',\n",
       " 'Be',\n",
       " 'ppl',\n",
       " 'Yes',\n",
       " 'SO',\n",
       " 'Next',\n",
       " 'fans',\n",
       " ':/',\n",
       " 'gone',\n",
       " \"'ve\",\n",
       " 'book',\n",
       " 'past',\n",
       " 'DAY',\n",
       " 'til',\n",
       " 'wear',\n",
       " 'Lohan',\n",
       " \"I'd\",\n",
       " 'Night',\n",
       " 'w/',\n",
       " '22',\n",
       " 'First',\n",
       " 'started',\n",
       " 'nothing',\n",
       " '24',\n",
       " 'anyone',\n",
       " 'girl',\n",
       " 'team',\n",
       " 'also',\n",
       " 'hour',\n",
       " \"doesn't\",\n",
       " 'dance',\n",
       " 'phone',\n",
       " 'hit',\n",
       " 'makes',\n",
       " 'yourself',\n",
       " 'OF',\n",
       " 'GET',\n",
       " 'Man',\n",
       " 'City',\n",
       " 'St',\n",
       " 'office',\n",
       " 'trying',\n",
       " 'making',\n",
       " 'busy',\n",
       " 'bit',\n",
       " 'wit',\n",
       " 'Its',\n",
       " 'TONIGHT',\n",
       " 'Lindsay',\n",
       " 're',\n",
       " 'does',\n",
       " 'Me',\n",
       " 'xx',\n",
       " 'YouTube',\n",
       " 'Long',\n",
       " '21',\n",
       " 'Had',\n",
       " 'April',\n",
       " \"'RT\",\n",
       " 'dream',\n",
       " 'wake',\n",
       " 'luck',\n",
       " 'stuff',\n",
       " 'OUT',\n",
       " 'Im',\n",
       " 'close',\n",
       " 'almost',\n",
       " 'lost',\n",
       " 'Christmas',\n",
       " 'open',\n",
       " 'End',\n",
       " 'omg',\n",
       " \"won't\",\n",
       " 'Super',\n",
       " 'second',\n",
       " 'less',\n",
       " 'least',\n",
       " 'From',\n",
       " 'ALL',\n",
       " 'la',\n",
       " 'July',\n",
       " 'FREE',\n",
       " 'monday',\n",
       " 'As',\n",
       " 'wish',\n",
       " 'Free',\n",
       " '28',\n",
       " 'forward',\n",
       " 'Nov',\n",
       " 'movie',\n",
       " 'Chicago',\n",
       " 'friend',\n",
       " 'pick',\n",
       " 'looks',\n",
       " 'which',\n",
       " 'guy',\n",
       " 'Follow',\n",
       " '16',\n",
       " 'takes',\n",
       " \"haven't\",\n",
       " 'finally',\n",
       " 'Great',\n",
       " 'OMG',\n",
       " 'move',\n",
       " 'far',\n",
       " 'IN',\n",
       " 'okay',\n",
       " 'Go',\n",
       " 'die',\n",
       " 'Pope',\n",
       " 'kill',\n",
       " 'many',\n",
       " 'wrong',\n",
       " 'mine',\n",
       " '5th',\n",
       " '6th',\n",
       " 'talk',\n",
       " 'FOR',\n",
       " 'John',\n",
       " 'Going',\n",
       " '50',\n",
       " 'mad',\n",
       " 'laugh',\n",
       " 'Thank',\n",
       " 'At',\n",
       " 'thanks',\n",
       " 'Did',\n",
       " 'driving',\n",
       " 'Last',\n",
       " 'D',\n",
       " 'missed',\n",
       " 'following',\n",
       " 'name',\n",
       " 'instead',\n",
       " 'Birthday',\n",
       " 'THIS',\n",
       " 'times',\n",
       " 'TIME',\n",
       " 'Before',\n",
       " 'turn',\n",
       " 'Like',\n",
       " 'ok',\n",
       " 'bring',\n",
       " '13',\n",
       " 'hey',\n",
       " 'ha',\n",
       " 'myself',\n",
       " 'room',\n",
       " \"didn't\",\n",
       " 'thank',\n",
       " 'Please',\n",
       " 'enjoy',\n",
       " 'money',\n",
       " 'album',\n",
       " 'plans',\n",
       " 'meeting',\n",
       " 'support',\n",
       " 'once',\n",
       " 'asked',\n",
       " 'blog',\n",
       " 'HAVE',\n",
       " 'MY',\n",
       " 'games',\n",
       " 'post',\n",
       " 'Days',\n",
       " '23',\n",
       " 'Music',\n",
       " 'due',\n",
       " 'Hey',\n",
       " 'problem',\n",
       " '19',\n",
       " 'Will',\n",
       " 'seeing',\n",
       " '4th',\n",
       " 'came',\n",
       " 'evening',\n",
       " 'sorry',\n",
       " 'working',\n",
       " 'buy',\n",
       " 'those',\n",
       " 'happen',\n",
       " 'change',\n",
       " 'yours',\n",
       " 'Find',\n",
       " 'waiting',\n",
       " 'rest',\n",
       " 'hours',\n",
       " \"'ll\",\n",
       " 'super',\n",
       " 'BUT',\n",
       " 'Yeah',\n",
       " 'lovely',\n",
       " 'else',\n",
       " 'boy',\n",
       " 'market',\n",
       " 'Favorite',\n",
       " 'supposed',\n",
       " 'gotta',\n",
       " 'wonderful',\n",
       " 'used',\n",
       " 'test',\n",
       " 'Boy',\n",
       " 'leaving',\n",
       " 'cuz',\n",
       " 'Edition',\n",
       " 'football',\n",
       " \"Don't\",\n",
       " 'About',\n",
       " 'running',\n",
       " 'means',\n",
       " 'Need',\n",
       " 'thinking',\n",
       " 'car',\n",
       " 'anything',\n",
       " 'water',\n",
       " 'LIKE',\n",
       " 'Oct',\n",
       " 'each',\n",
       " 'probably',\n",
       " 'practice',\n",
       " 'fast',\n",
       " 'que',\n",
       " 'favorite',\n",
       " 'easy',\n",
       " 'mind',\n",
       " 'Sorry',\n",
       " 'Up',\n",
       " 'Are',\n",
       " 'Can',\n",
       " 'while',\n",
       " 'celebrate',\n",
       " 'news',\n",
       " 'town',\n",
       " 'See',\n",
       " 'FOLLOW',\n",
       " 'Photo',\n",
       " 'NEW',\n",
       " 'ass',\n",
       " 'case',\n",
       " ':P',\n",
       " 'fucking',\n",
       " '......',\n",
       " 'cause',\n",
       " 'wants',\n",
       " 'Club',\n",
       " 'Videos',\n",
       " 'care',\n",
       " 'summer',\n",
       " 'sa',\n",
       " ';-)',\n",
       " '31',\n",
       " 'Gonna',\n",
       " 'spend',\n",
       " 'en',\n",
       " 'drink',\n",
       " 'B',\n",
       " 'XD',\n",
       " 'Big',\n",
       " 'shows',\n",
       " '~',\n",
       " 'mar',\n",
       " 'By',\n",
       " 'five',\n",
       " 'IF',\n",
       " 'met',\n",
       " 'cry',\n",
       " 'such',\n",
       " 'couple',\n",
       " 'mo',\n",
       " 'Year',\n",
       " 'World',\n",
       " 'True',\n",
       " 'girls',\n",
       " 'een',\n",
       " 'definitely',\n",
       " 'send',\n",
       " '27',\n",
       " 'top',\n",
       " 'called',\n",
       " 'em',\n",
       " 'Out',\n",
       " 'xD',\n",
       " 'rain',\n",
       " 'August',\n",
       " 'Morning',\n",
       " 'along',\n",
       " '14th',\n",
       " '???',\n",
       " '14',\n",
       " 'picture',\n",
       " 'idea',\n",
       " 'Ah',\n",
       " 'hopefully',\n",
       " 'UP',\n",
       " 't',\n",
       " 'family',\n",
       " 'near',\n",
       " 'heard',\n",
       " \"i'll\",\n",
       " 'moment',\n",
       " \"wasn't\",\n",
       " 'dreams',\n",
       " \"isn't\",\n",
       " 'able',\n",
       " 'round',\n",
       " ':-)',\n",
       " 'shoot',\n",
       " 'email',\n",
       " 'London',\n",
       " 'taking',\n",
       " '...:',\n",
       " 'between',\n",
       " 'beautiful',\n",
       " 'date',\n",
       " 'without',\n",
       " \"we'll\",\n",
       " 'fresh',\n",
       " 'drug',\n",
       " 'local',\n",
       " 'sat',\n",
       " 'line',\n",
       " 'radio',\n",
       " 'WE',\n",
       " 'food',\n",
       " '!!!!!',\n",
       " 'funny',\n",
       " 'enter',\n",
       " 'Live',\n",
       " 'SCHOOL',\n",
       " 'Sep',\n",
       " 'info',\n",
       " 'turns',\n",
       " 'happens',\n",
       " 'Maybe',\n",
       " 'TALK',\n",
       " 'York',\n",
       " 'thats',\n",
       " 'lots',\n",
       " 'keep',\n",
       " 'd',\n",
       " 'starting',\n",
       " 'Another',\n",
       " 'Storm',\n",
       " 'epic',\n",
       " 'TODAY',\n",
       " '9',\n",
       " 'Drama',\n",
       " 'bout',\n",
       " 'wanted',\n",
       " \"ain't\",\n",
       " 'November',\n",
       " 'alone',\n",
       " 'stores',\n",
       " 'Got',\n",
       " '@LightCMS',\n",
       " '//',\n",
       " 'form',\n",
       " 'Festival',\n",
       " 'beat',\n",
       " 'BE',\n",
       " 'photo',\n",
       " 'THAT',\n",
       " 'seconds',\n",
       " '26',\n",
       " 'Mark',\n",
       " 'hang',\n",
       " 'Join',\n",
       " 'online',\n",
       " 'nap',\n",
       " 'trip',\n",
       " 'removed',\n",
       " 'fall',\n",
       " 'Beautiful',\n",
       " 'Bowl',\n",
       " 'DO',\n",
       " 'tried',\n",
       " 'Make',\n",
       " '#ZodiacFacts',\n",
       " 'part',\n",
       " 'bought',\n",
       " 'perfect',\n",
       " 'event',\n",
       " 'living',\n",
       " 'Win',\n",
       " 'final',\n",
       " 'Back',\n",
       " 'Woman',\n",
       " 'finish',\n",
       " 'West',\n",
       " 'Weekend',\n",
       " 'wearing',\n",
       " 'didnt',\n",
       " 'reason',\n",
       " 'High',\n",
       " 'service',\n",
       " 'eat',\n",
       " \":'\",\n",
       " 'truck',\n",
       " 'e',\n",
       " 'seriously',\n",
       " 'Gotta',\n",
       " 'ice',\n",
       " 'Best',\n",
       " 'afternoon',\n",
       " 'under',\n",
       " 'wonder',\n",
       " 'front',\n",
       " 'Five',\n",
       " 'Michael',\n",
       " \"'re\",\n",
       " 'lmao',\n",
       " 'slow',\n",
       " 'seems',\n",
       " 'r',\n",
       " 'gym',\n",
       " 'Anniversary',\n",
       " 'ive',\n",
       " 'Tour',\n",
       " 'learn',\n",
       " 'HAPPY',\n",
       " 'lil',\n",
       " 'hurt',\n",
       " 'plan',\n",
       " 'told',\n",
       " 'short',\n",
       " 'justin',\n",
       " 'NYC',\n",
       " 'faces',\n",
       " 'enough',\n",
       " 'goin',\n",
       " 'facebook',\n",
       " 'ill',\n",
       " '3D',\n",
       " 'lunch',\n",
       " 'Pretty',\n",
       " 'Only',\n",
       " '7th',\n",
       " 'won',\n",
       " 'cute',\n",
       " 'child',\n",
       " 'choose',\n",
       " 'guess',\n",
       " 'Damn',\n",
       " 'Pisces',\n",
       " '@Sexstrology',\n",
       " '25th',\n",
       " 'iPad',\n",
       " 'link',\n",
       " 'Top',\n",
       " 'b',\n",
       " 'everyday',\n",
       " 'played',\n",
       " 'together',\n",
       " 'official',\n",
       " 'real',\n",
       " \"You're\",\n",
       " 'write',\n",
       " 'liked',\n",
       " 'special',\n",
       " 'S',\n",
       " 'seem',\n",
       " 'wrote',\n",
       " 'AT',\n",
       " 'NO',\n",
       " '!\"',\n",
       " 'group',\n",
       " 'Sat',\n",
       " 'ik',\n",
       " 'air',\n",
       " 'spent',\n",
       " \"you'll\",\n",
       " 'words',\n",
       " 'talking',\n",
       " 'hold',\n",
       " 'save',\n",
       " 'record',\n",
       " 'tea',\n",
       " 'upon',\n",
       " 'yea',\n",
       " 'al',\n",
       " 'Excuses',\n",
       " 'gave',\n",
       " '::',\n",
       " 'fire',\n",
       " 'posted',\n",
       " 'million',\n",
       " 'loved',\n",
       " 'age',\n",
       " \"...'\",\n",
       " 'break',\n",
       " 'ones',\n",
       " 'Game',\n",
       " ...]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('crf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50f95997f0c4282c02b70176f376c0566cd16eb2ce4c64de3c8feb9eb1d0f610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
